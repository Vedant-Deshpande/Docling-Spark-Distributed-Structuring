# k8s/spark-submit-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: docling-spark-job
  namespace: docling-spark
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: docling-spark
        component: driver
    spec:
      serviceAccountName: spark-driver
      restartPolicy: Never
      
      # Add image pull secret if using private Quay repo
      imagePullSecrets:
      - name: rishasin-pull-secret
      
      containers:
      - name: spark-submit
        image: quay.io/rishasin/docling-spark:latest
        imagePullPolicy: Always
        
        command: ["/opt/spark/bin/spark-submit"]
        args:
        - --master
        - k8s://https://kubernetes.default.svc:443
        - --deploy-mode
        - client
        - --name
        - docling-pdf-processing
        - --conf
        - spark.kubernetes.namespace=docling-spark
        - --conf
        - spark.kubernetes.authenticate.driver.serviceAccountName=spark-driver
        - --conf
        - spark.kubernetes.container.image=quay.io/rishasin/docling-spark:latest
        - --conf
        - spark.kubernetes.container.image.pullPolicy=Always
        - --conf
        - spark.kubernetes.container.image.pullSecrets=rishasin-pull-secret
        - --conf
        - spark.executor.instances=5
        - --conf
        - spark.executor.memory=4g
        - --conf
        - spark.executor.cores=2
        - --conf
        - spark.driver.memory=4g
        - --conf
        - spark.driver.cores=2
        - --conf
        - spark.kubernetes.driver.volumes.persistentVolumeClaim.input-volume.options.claimName=docling-input-pvc
        - --conf
        - spark.kubernetes.driver.volumes.persistentVolumeClaim.input-volume.mount.path=/app/input
        - --conf
        - spark.kubernetes.driver.volumes.persistentVolumeClaim.output-volume.options.claimName=docling-output-pvc
        - --conf
        - spark.kubernetes.driver.volumes.persistentVolumeClaim.output-volume.mount.path=/app/output
        - --conf
        - spark.kubernetes.executor.volumes.persistentVolumeClaim.input-volume.options.claimName=docling-input-pvc
        - --conf
        - spark.kubernetes.executor.volumes.persistentVolumeClaim.input-volume.mount.path=/app/input
        - --conf
        - spark.kubernetes.executor.volumes.persistentVolumeClaim.output-volume.options.claimName=docling-output-pvc
        - --conf
        - spark.kubernetes.executor.volumes.persistentVolumeClaim.output-volume.mount.path=/app/output
        - --conf
        - spark.dynamicAllocation.enabled=true
        - --conf
        - spark.dynamicAllocation.minExecutors=2
        - --conf
        - spark.dynamicAllocation.maxExecutors=10
        - --conf
        - spark.dynamicAllocation.shuffleTracking.enabled=true
        - --conf
        - spark.jars.ivy=/tmp/.ivy2
        - --py-files
        - /app/scripts/docling_module/__init__.py,/app/scripts/docling_module/processor.py
        - /app/scripts/run_spark_job.py
        
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "6Gi"
            cpu: "3"
        
        env:
        - name: HOME
          value: /tmp
        - name: SPARK_LOCAL_DIRS
          value: /tmp
        - name: IVY_HOME
          value: /tmp/.ivy2
        
        volumeMounts:
        - name: input-volume
          mountPath: /app/input
        - name: output-volume
          mountPath: /app/output
      
      volumes:
      - name: input-volume
        persistentVolumeClaim:
          claimName: docling-input-pvc
      - name: output-volume
        persistentVolumeClaim:
          claimName: docling-output-pvc